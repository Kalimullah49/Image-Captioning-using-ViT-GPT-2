{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsK2_BMGrMwt"
   },
   "source": [
    "## Image Caption Generator \n",
    "\n",
    "We are going to use Transformers model to generate caption from an Image.\n",
    "\n",
    "### Installation\n",
    "\n",
    "\n",
    "\n",
    "1.   Transformers\n",
    "2.   Pytorch\n",
    "3. Image \n",
    "\n",
    "For installation, please do pip install package_name\n",
    "\n",
    "In Colab, Pytorch comes preinstalled and same goes with PIL for Image. You will only need to install **transformers** from Huggingface.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYLlSVpnnpjr",
    "outputId": "5ed9e710-7b4e-4f8e-f6b3-5712b99e2f4e"
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qrKcl2XWnaT8"
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-jYh878nnpe",
    "outputId": "156ef6f3-4441-4e44-d96c-57060797eb66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a white swan swimming on top of a body of water']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "def predict_step(image_paths):\n",
    "  images = []\n",
    "  for image_path in image_paths:\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "      i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "\n",
    "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "  pixel_values = pixel_values.to(device)\n",
    "\n",
    "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  return preds\n",
    "  \n",
    "predict_step(['Sample.jpg']) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio User Interface for Image and Video Captioning\n",
    "\n",
    "This section adds a Gradio interface to your project, allowing users to upload images or videos and receive captions generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gradio if not already installed\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\kalim\\AppData\\Local\\Temp\\ipykernel_1196\\2926413555.py\", line 27, in predict_video\n",
      "    return predict_image(frame_rgb)\n",
      "  File \"C:\\Users\\kalim\\AppData\\Local\\Temp\\ipykernel_1196\\2926413555.py\", line 7, in predict_image\n",
      "    pil_image = Image.fromarray(image)\n",
      "NameError: name 'Image' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\kalim\\AppData\\Local\\Temp\\ipykernel_1196\\2926413555.py\", line 7, in predict_image\n",
      "    pil_image = Image.fromarray(image)\n",
      "NameError: name 'Image' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\kalim\\miniconda3\\envs\\ML\\lib\\site-packages\\gradio\\utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\kalim\\AppData\\Local\\Temp\\ipykernel_1196\\2926413555.py\", line 7, in predict_image\n",
      "    pil_image = Image.fromarray(image)\n",
      "NameError: name 'Image' is not defined\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def predict_image(image):\n",
    "    # Convert numpy array to PIL Image\n",
    "    pil_image = Image.fromarray(image)\n",
    "    if pil_image.mode != \"RGB\":\n",
    "        pil_image = pil_image.convert(mode=\"RGB\")\n",
    "    pixel_values = feature_extractor(images=[pil_image], return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return preds[0].strip()\n",
    "\n",
    "def predict_video(video):\n",
    "    # Read video frames and select a representative frame (e.g., the middle frame)\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    mid_frame = frame_count // 2\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        return \"Could not read video frame.\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return predict_image(frame_rgb)\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"blue\"), css=\".gradio-container {max-width: 1600px !important;} .gradio-container {width: 100% !important;}\") as demo:\n",
    "    gr.Markdown(\"# Image & Video Caption Generator\")\n",
    "    gr.Markdown(\"Upload an image or a video to generate a caption using a HuggingFace transformer model.\")\n",
    "    with gr.Tab(\"Image\"):\n",
    "        image_input = gr.Image(type=\"numpy\", label=\"Upload Image\")\n",
    "        image_output = gr.Textbox(label=\"Generated Caption\")\n",
    "        image_btn = gr.Button(\"Generate Caption\")\n",
    "        image_btn.click(fn=predict_image, inputs=image_input, outputs=image_output)\n",
    "    with gr.Tab(\"Video\"):\n",
    "        video_input = gr.Video(label=\"Upload Video\")\n",
    "        video_output = gr.Textbox(label=\"Generated Caption\")\n",
    "        video_btn = gr.Button(\"Generate Caption\")\n",
    "        video_btn.click(fn=predict_video, inputs=video_input, outputs=video_output)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "caption.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
